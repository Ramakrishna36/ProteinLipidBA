{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b564d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# This is to force the path to be on the same level as the dl_ba folder\n",
    "sys.path.append(\"../..\") \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "\n",
    "from balm import common_utils\n",
    "from balm.models.utils import load_trained_model, load_pretrained_pkd_bounds\n",
    "from balm.configs import Configs\n",
    "from balm.models import LipidBALM\n",
    "\n",
    "DEVICE = \"cpu\"  # Change to \"cuda\" if using GPU\n",
    "\n",
    "# Load Pretrained LipidBALM model\n",
    "config_filepath = \"../../configs/lipidbalm_peft.yaml\"\n",
    "configs = Configs(**common_utils.load_yaml(config_filepath))\n",
    "\n",
    "# Load the model\n",
    "model = LipidBALM(configs.model_configs)\n",
    "model = load_trained_model(model, configs.model_configs, is_training=False)\n",
    "model.to(DEVICE)  # Use this to move the model to the specified device (CPU or GPU)\n",
    "\n",
    "model.eval()\n",
    "# Pretrained binding affinity lower and upper bounds\n",
    "pkd_lower_bound, pkd_upper_bound = load_pretrained_pkd_bounds(configs.model_configs.checkpoint_path)\n",
    "\n",
    "# Load the tokenizers\n",
    "protein_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    configs.model_configs.protein_model_name_or_path\n",
    ")\n",
    "lipid_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    configs.model_configs.lipid_model_name_or_path\n",
    ")\n",
    "\n",
    "# Custom Data: load your combined_binding_data.csv  \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_binding_data.csv\")\n",
    "\n",
    "# Examine the first few rows\n",
    "df.head(5)\n",
    "\n",
    "# Zero shot predictions with pretrained model\n",
    "start = time.time()\n",
    "predictions = []\n",
    "labels = []\n",
    "for _, sample in df.iterrows():\n",
    "    # Prepare input\n",
    "    protein_inputs = protein_tokenizer(sample[\"ProteinSequence\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    lipid_inputs = lipid_tokenizer(sample[\"LipidSMILES\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs = {\n",
    "        \"protein_input_ids\": protein_inputs[\"input_ids\"],\n",
    "        \"protein_attention_mask\": protein_inputs[\"attention_mask\"],\n",
    "        \"lipid_input_ids\": lipid_inputs[\"input_ids\"],\n",
    "        \"lipid_attention_mask\": lipid_inputs[\"attention_mask\"],\n",
    "    }\n",
    "    prediction = model(inputs)[\"cosine_similarity\"]\n",
    "    prediction = model.cosine_similarity_to_pkd(prediction, pkd_upper_bound=pkd_upper_bound, pkd_lower_bound=pkd_lower_bound)\n",
    "    label = torch.tensor([sample[\"BindingAffinityValue\"]])\n",
    "\n",
    "    print(f\"Predicted binding affinity: {prediction.item()} | True binding affinity: {label.item()}\")\n",
    "    predictions.append(prediction.item())\n",
    "    labels.append(label.item())\n",
    "print(f\"Time taken for {len(df)} protein-lipid pairs: {time.time() - start}\")\n",
    "\n",
    "# Visualize results\n",
    "from balm.metrics import get_ci, get_pearson, get_rmse, get_spearman\n",
    "import seaborn as sns\n",
    "\n",
    "rmse = get_rmse(torch.tensor(labels), torch.tensor(predictions))\n",
    "pearson = get_pearson(torch.tensor(labels), torch.tensor(predictions))\n",
    "spearman = get_spearman(torch.tensor(labels), torch.tensor(predictions))\n",
    "ci = get_ci(torch.tensor(labels), torch.tensor(predictions))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")\n",
    "print(f\"CI: {ci}\")\n",
    "\n",
    "ax = sns.regplot(x=labels, y=predictions)\n",
    "ax.set_title(f\"Protein-Lipid Binding Affinity Prediction\")\n",
    "ax.set_xlabel(r\"Experimental Binding Affinity\")\n",
    "ax.set_ylabel(r\"Predicted Binding Affinity\")\n",
    "\n",
    "# Few shot training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(df, train_size=0.2, random_state=1234)\n",
    "\n",
    "# Define a function that applies the cosine similarity conversion to a single example\n",
    "# This is VERY IMPORTANT since LipidBALM uses cosine similarity\n",
    "def add_cosine_similarity(example, pkd_upper_bound, pkd_lower_bound):\n",
    "    example['cosine_similarity'] = (\n",
    "        (example['BindingAffinityValue'] - pkd_lower_bound)\n",
    "        / (pkd_upper_bound - pkd_lower_bound)\n",
    "        * 2\n",
    "        - 1\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# Use map to apply the function across the entire dataset\n",
    "train_data = train_data.apply(lambda x: add_cosine_similarity(x, pkd_upper_bound, pkd_lower_bound), axis=1)\n",
    "test_data = test_data.apply(lambda x: add_cosine_similarity(x, pkd_upper_bound, pkd_lower_bound), axis=1)\n",
    "\n",
    "print(f\"Number of train data: {len(train_data)}\")\n",
    "print(f\"Number of test data: {len(test_data)}\")\n",
    "\n",
    "# Initialize model for fine-tuning\n",
    "model = LipidBALM(configs.model_configs)\n",
    "model = load_trained_model(model, configs.model_configs, is_training=True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Training loop\n",
    "from torch.optim import AdamW\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "optimizer = AdamW(\n",
    "    params=[\n",
    "        param\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    ],\n",
    "    lr=configs.model_configs.model_hyperparameters.learning_rate,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0  # To track the loss for each epoch\n",
    "\n",
    "    for _, sample in train_data.iterrows():\n",
    "        # Prepare input\n",
    "        protein_inputs = protein_tokenizer(sample[\"ProteinSequence\"], return_tensors=\"pt\").to(DEVICE)\n",
    "        lipid_inputs = lipid_tokenizer(sample[\"LipidSMILES\"], return_tensors=\"pt\").to(DEVICE)\n",
    "        # Move labels to the appropriate device and ensure it's a tensor\n",
    "        labels = torch.tensor([sample[\"cosine_similarity\"]], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        inputs = {\n",
    "            \"protein_input_ids\": protein_inputs[\"input_ids\"],\n",
    "            \"protein_attention_mask\": protein_inputs[\"attention_mask\"],\n",
    "            \"lipid_input_ids\": lipid_inputs[\"input_ids\"],\n",
    "            \"lipid_attention_mask\": lipid_inputs[\"attention_mask\"],\n",
    "            \"labels\": labels,  # Add labels for training\n",
    "        }\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get loss\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # Zero out the gradients to avoid accumulation\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Log the loss for this epoch\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete! Time taken: \", time.time() - start)\n",
    "\n",
    "# Test the fine-tuned model\n",
    "model = model.eval()\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "for _, sample in test_data.iterrows():\n",
    "    # Prepare input\n",
    "    protein_inputs = protein_tokenizer(sample[\"ProteinSequence\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    lipid_inputs = lipid_tokenizer(sample[\"LipidSMILES\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs = {\n",
    "        \"protein_input_ids\": protein_inputs[\"input_ids\"],\n",
    "        \"protein_attention_mask\": protein_inputs[\"attention_mask\"],\n",
    "        \"lipid_input_ids\": lipid_inputs[\"input_ids\"],\n",
    "        \"lipid_attention_mask\": lipid_inputs[\"attention_mask\"],\n",
    "    }\n",
    "    prediction = model(inputs)[\"cosine_similarity\"]\n",
    "    prediction = model.cosine_similarity_to_pkd(prediction, pkd_upper_bound=pkd_upper_bound, pkd_lower_bound=pkd_lower_bound)\n",
    "    label = torch.tensor([sample[\"BindingAffinityValue\"]])\n",
    "\n",
    "    print(f\"Predicted binding affinity: {prediction.item()} | True binding affinity: {label.item()}\")\n",
    "    predictions.append(prediction.item())\n",
    "    labels.append(label.item())\n",
    "print(f\"Time taken for {len(test_data)} protein-lipid pairs: {time.time() - start}\")\n",
    "\n",
    "# Visualize results after fine-tuning\n",
    "rmse = get_rmse(torch.tensor(labels), torch.tensor(predictions))\n",
    "pearson = get_pearson(torch.tensor(labels), torch.tensor(predictions))\n",
    "spearman = get_spearman(torch.tensor(labels), torch.tensor(predictions))\n",
    "ci = get_ci(torch.tensor(labels), torch.tensor(predictions))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Pearson: {pearson}\")\n",
    "print(f\"Spearman: {spearman}\")\n",
    "print(f\"CI: {ci}\")\n",
    "\n",
    "ax = sns.regplot(x=labels, y=predictions)\n",
    "ax.set_title(f\"Fine-tuned Protein-Lipid Model\")\n",
    "ax.set_xlabel(r\"Experimental Binding Affinity\")\n",
    "ax.set_ylabel(r\"Predicted Binding Affinity\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
